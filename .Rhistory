# Load dataset
data <- read.csv("cleaned_tmdb_data_final.csv", stringsAsFactors = FALSE)
# Classify popularity into High, Medium, and Low
data$popularity_level <- cut(data$popularity, breaks = c(-Inf, quantile(data$popularity, c(1/3, 2/3)), Inf), labels = c("Low", "Medium", "High"))
# Remove popularity column
data <- data[, !(names(data) %in% c("popularity"))]
# Split data into training and testing sets
set.seed(123) # For reproducibility
trainIndex <- createDataPartition(data$popularity_level, p = 0.8, list = FALSE)
train_data <- data[trainIndex, ]
test_data <- data[-trainIndex, ]
# Train AdaBoost classifier
boost_model <- boosting(popularity_level ~ .,
data = train_data,
mfinal = 200,
boos = TRUE)
# Predict on test data
predictions <- predict(boost_model, newdata = test_data)
# Evaluate the model
confusionMatrix(factor(predictions$class), test_data$popularity_level)
# precision_low <-
# recall_low <-
#
# precision_medium <-
# recall_medium <-
#
# precision_high <-
# recall_high <-
#
# f1_score_low <- 2 * (precision_low * recall_low) / (precision_low + recall_low)
# f1_score_medium <- 2 * (precision_medium * recall_medium) / (precision_medium + recall_medium)
# f1_score_high <- 2 * (precision_medium * precision_high) / (precision_medium + precision_high)
# Load required libraries
library(tidyverse)
library(caret)
library(e1071)
# Load dataset
data <- read.csv("cleaned_tmdb_data_final.csv", stringsAsFactors = FALSE)
# Classify popularity into High, Medium, and Low
data$popularity_level <- cut(data$popularity, breaks = c(-Inf, quantile(data$popularity, c(1/3, 2/3)), Inf), labels = c("Low", "Medium", "High"))
# Remove popularity column
data <- data[, !(names(data) %in% c("popularity"))]
# Split data into training and testing sets
set.seed(123) # For reproducibility
trainIndex <- createDataPartition(data$popularity_level, p = 0.8, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]
# Train Naive Bayes classifier
model <- naiveBayes(popularity_level ~ .,
data = trainData,
laplace = 0)
# Predict using the trained model
predictions <- predict(model, testData)
# Evaluate the model
confusionMatrix(predictions, testData$popularity_level)
# precision_low <-
# recall_low <-
#
# precision_medium <-
# recall_medium <-
#
# precision_high <-
# recall_high <-
#
# f1_score_low <- 2 * (precision_low * recall_low) / (precision_low + recall_low)
# f1_score_medium <- 2 * (precision_medium * recall_medium) / (precision_medium + recall_medium)
# f1_score_high <- 2 * (precision_medium * precision_high) / (precision_medium + precision_high)
# Load libraries
library(tidyverse)
library(caret)
# Load dataset
data <- read.csv("cleaned_tmdb_data_final.csv", stringsAsFactors = FALSE)
# Classify popularity into High, Medium, and Low on basis of quantile
data$popularity_level <- cut(data$popularity, breaks = c(-Inf, quantile(data$popularity, c(1/3, 2/3)), Inf), labels = c("Low", "Medium", "High"))
# Remove popularity column
data <- data[, !(names(data) %in% c("popularity"))]
# Split data into training and testing sets
set.seed(123) # For reproducibility
train_index <- createDataPartition(data$popularity_level, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Train model
model <- train(popularity_level ~ .,
data = train_data,
method = "knn")
# Make predictions on test set
predictions <- predict(model, test_data)
# Confusion matrix
confusionMatrix(predictions, test_data$popularity_level)
# precision_low <-
# recall_low <-
#
# precision_medium <-
# recall_medium <-
#
# precision_high <-
# recall_high <-
#
# f1_score_low <- 2 * (precision_low * recall_low) / (precision_low + recall_low)
# f1_score_medium <- 2 * (precision_medium * recall_medium) / (precision_medium + recall_medium)
# f1_score_high <- 2 * (precision_medium * precision_high) / (precision_medium + precision_high)
# Load required libraries
library(tidyverse)
library(caret)
library(rpart)
library(rpart.plot)
# Load dataset
data <- read.csv("cleaned_tmdb_data_final.csv", stringsAsFactors = FALSE)
# Classify popularity into High, Medium, and Low
data$popularity_level <- cut(data$popularity, breaks = c(-Inf, quantile(data$popularity, c(1/3, 2/3)), Inf), labels = c("Low", "Medium", "High"))
#quantiles <- quantile(data$popularity, c(1/3, 2/3))
# Remove popularity column
data <- data[, !(names(data) %in% c("popularity"))]
# Split data into training and testing sets
set.seed(123) # For reproducibility
train_index <- createDataPartition(data$popularity_level, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Decision tree model
model <- rpart(popularity_level ~ .,
data = train_data,
method = "class",
cp = 0.001,
minsplit = 25,
minbucket = 5,
maxdepth = 10,
maxcompete = 4,
xval = 8)
# Plot the decision tree
#rpart.plot(model, main = "Decision Tree")
#prp(model)
#plotcp(model)
# Predictions on test data
predictions <- predict(model, test_data, type = "class")
# Confusion matrix
confusionMatrix(predictions, test_data$popularity_level)
#
# precision_low <-
# recall_low <-
#
# precision_medium <-
# recall_medium <-
#
# precision_high <-
# recall_high <-
#
# f1_score_low <- 2 * (precision_low * recall_low) / (precision_low + recall_low)
# f1_score_medium <- 2 * (precision_medium * recall_medium) / (precision_medium + recall_medium)
# f1_score_high <- 2 * (precision_medium * precision_high) / (precision_medium + precision_high)
# Load libraries
library(tidyverse)
library(caret)
library(randomForest)
# Load dataset
data <- read.csv("cleaned_tmdb_data_final.csv", stringsAsFactors = FALSE)
# Classify popularity into High, Medium, and Low
data$popularity_level <- cut(data$popularity, breaks = c(-Inf, quantile(data$popularity, c(1/3, 2/3)), Inf), labels = c("Low", "Medium", "High"))
# Remove popularity column
data <- data[, !(names(data) %in% c("popularity"))]
# Split data into training and testing sets
set.seed(123) # For reproducibility
train_index <- createDataPartition(data$popularity_level, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Train model
rf_model <- randomForest(popularity_level ~ .,
data = train_data,
ntree = 600,
mtry = 1,
nodesize = 12,
maxnodes = 20,
maxdepth = 25)
#   - ntree: Number of trees in the forest
#   - mtry: Number of variables randomly sampled as candidates at each split
#   - nodesize: Minimum size of terminal nodes
#   - maxnodes: Maximum number of terminal nodes
#   - maxdepth: Maximum depth of the tree
# Make predictions
rf_pred <- predict(rf_model, test_data)
# Confusion matrix
confusionMatrix(rf_pred, test_data$popularity_level)
# precision_low <-
# recall_low <-
#
# precision_medium <-
# recall_medium <-
#
# precision_high <-
# recall_high <-
#
# f1_score_low <- 2 * (precision_low * recall_low) / (precision_low + recall_low)
# f1_score_medium <- 2 * (precision_medium * recall_medium) / (precision_medium + recall_medium)
# f1_score_high <- 2 * (precision_medium * precision_high) / (precision_medium + precision_high)
precision_low <- 0.8585
recall_low <- 0.9113
precision_medium <- 0.8238
recall_medium <- 0.7338
precision_high <- 0.8795
recall_high <- 0.9215
f1_score_low <- 2 * (precision_low * recall_low) / (precision_low + recall_low)
f1_score_medium <- 2 * (precision_medium * recall_medium) / (precision_medium + recall_medium)
f1_score_high <- 2 * (precision_medium * precision_high) / (precision_medium + precision_high)
cat("f1_score_low : ",f1_score_low)
cat("f1_score_medium : ",f1_score_medium)
cat("f1_score_high : ",f1_score_high)
precision_low <- 0.6828
recall_low <- 	0.7201
precision_medium <-0.4870
recall_medium <-  	0.4471
precision_high <-0.6877
recall_high <-		0.7065
f1_score_low <- 2 * (precision_low * recall_low) / (precision_low + recall_low)
f1_score_medium <- 2 * (precision_medium * recall_medium) / (precision_medium + recall_medium)
f1_score_high <- 2 * (precision_medium * precision_high) / (precision_medium + precision_high)
cat("f1_score_low : ",f1_score_low)
cat("f1_score_medium : ",f1_score_medium)
cat("f1_score_high : ",f1_score_high)
precision_low <- 0.6828
recall_low <- 	0.7201
precision_medium <-0.4870
recall_medium <-  	0.4471
precision_high <-0.6877
recall_high <-		0.7065
f1_score_low <- 2 * (precision_low * recall_low) / (precision_low + recall_low)
f1_score_medium <- 2 * (precision_medium * recall_medium) / (precision_medium + recall_medium)
f1_score_high <- 2 * (precision_medium * precision_high) / (precision_medium + precision_high)
cat("f1_score_low : ",f1_score_low)
cat("f1_score_medium : ",f1_score_medium)
cat("f1_score_high : ",f1_score_high)
precision_low <-0.6178
recall_low <- 	0.9215
precision_medium <-0.5000
recall_medium <-0.3720
precision_high <-0.8929
recall_high <-	0.6826
f1_score_low <- 2 * (precision_low * recall_low) / (precision_low + recall_low)
f1_score_medium <- 2 * (precision_medium * recall_medium) / (precision_medium + recall_medium)
f1_score_high <- 2 * (precision_medium * precision_high) / (precision_medium + precision_high)
cat("f1_score_low : ",f1_score_low)
cat("f1_score_medium : ",f1_score_medium)
cat("f1_score_high : ",f1_score_high)
precision_low <-0.8377
recall_low <-	0.8635
precision_medium <-0.7361
recall_medium <-	0.7235
precision_high <-0.8754
recall_high <-	0.8635
f1_score_low <- 2 * (precision_low * recall_low) / (precision_low + recall_low)
f1_score_medium <- 2 * (precision_medium * recall_medium) / (precision_medium + recall_medium)
f1_score_high <- 2 * (precision_medium * precision_high) / (precision_medium + precision_high)
cat("f1_score_low : ",f1_score_low)
cat("f1_score_medium : ",f1_score_medium)
cat("f1_score_high : ",f1_score_high)
precision_low <-0.8248
recall_low <-   	0.9317
precision_medium <-0.7962
recall_medium <-  	0.7201
precision_high <-0.9046
recall_high <-0.8737
f1_score_low <- 2 * (precision_low * recall_low) / (precision_low + recall_low)
f1_score_medium <- 2 * (precision_medium * recall_medium) / (precision_medium + recall_medium)
f1_score_high <- 2 * (precision_medium * precision_high) / (precision_medium + precision_high)
cat("f1_score_low : ",f1_score_low)
cat("f1_score_medium : ",f1_score_medium)
cat("f1_score_high : ",f1_score_high)
precision_low <-0.8742
recall_low <-   	0.9010
precision_medium <-0.8107
recall_medium <-   	0.7747
precision_high <-0.8956
recall_high <-0.9078
f1_score_low <- 2 * (precision_low * recall_low) / (precision_low + recall_low)
f1_score_medium <- 2 * (precision_medium * recall_medium) / (precision_medium + recall_medium)
f1_score_high <- 2 * (precision_medium * precision_high) / (precision_medium + precision_high)
cat("f1_score_low : ",f1_score_low)
cat("f1_score_medium : ",f1_score_medium)
cat("f1_score_high : ",f1_score_high)
# Load required libraries
library(tidyverse)
library(caret)
library(rpart)
library(rpart.plot)
# Load dataset
data <- read.csv("cleaned_tmdb_data_final.csv", stringsAsFactors = FALSE)
# Classify popularity into High, Medium, and Low
data$popularity_level <- cut(data$popularity, breaks = c(-Inf, quantile(data$popularity, c(1/3, 2/3)), Inf), labels = c("Low", "Medium", "High"))
#quantiles <- quantile(data$popularity, c(1/3, 2/3))
# Remove popularity column
data <- data[, !(names(data) %in% c("popularity"))]
# Split data into training and testing sets
set.seed(123) # For reproducibility
train_index <- createDataPartition(data$popularity_level, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Decision tree model
model <- rpart(popularity_level ~ .,
data = train_data,
method = "class",
cp = 0.001,
minsplit = 25,
minbucket = 5,
maxdepth = 10,
maxcompete = 4,
xval = 8)
# Plot the decision tree
#rpart.plot(model, main = "Decision Tree")
#prp(model)
#plotcp(model)
# Predictions on test data
predictions <- predict(model, test_data, type = "class")
# Confusion matrix
confusionMatrix(predictions, test_data$popularity_level)
precision_low <- 0.8585
recall_low <- 0.9113
precision_medium <- 0.8238
recall_medium <- 0.7338
precision_high <- 0.8795
recall_high <- 0.9215
f1_score_low <- 2 * (precision_low * recall_low) / (precision_low + recall_low)
f1_score_medium <- 2 * (precision_medium * recall_medium) / (precision_medium + recall_medium)
f1_score_high <- 2 * (precision_medium * precision_high) / (precision_medium + precision_high)
cat("f1_score_low : ",f1_score_low)
cat("f1_score_medium : ",f1_score_medium)
cat("f1_score_high : ",f1_score_high)
# Load libraries
library(tidyverse)
library(caret)
# Load dataset
data <- read.csv("cleaned_tmdb_data_final.csv", stringsAsFactors = FALSE)
# Classify popularity into High, Medium, and Low on basis of quantile
data$popularity_level <- cut(data$popularity, breaks = c(-Inf, quantile(data$popularity, c(1/3, 2/3)), Inf), labels = c("Low", "Medium", "High"))
# Remove popularity column
data <- data[, !(names(data) %in% c("popularity"))]
# Split data into training and testing sets
set.seed(123) # For reproducibility
train_index <- createDataPartition(data$popularity_level, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Train model
model <- train(popularity_level ~ .,
data = train_data,
method = "knn")
# Make predictions on test set
predictions <- predict(model, test_data)
# Confusion matrix
confusionMatrix(predictions, test_data$popularity_level)
precision_low <- 0.6828
recall_low <- 	0.7201
precision_medium <-0.4870
recall_medium <-  	0.4471
precision_high <-0.6877
recall_high <-		0.7065
f1_score_low <- 2 * (precision_low * recall_low) / (precision_low + recall_low)
f1_score_medium <- 2 * (precision_medium * recall_medium) / (precision_medium + recall_medium)
f1_score_high <- 2 * (precision_medium * precision_high) / (precision_medium + precision_high)
cat("f1_score_low : ",f1_score_low)
cat("f1_score_medium : ",f1_score_medium)
cat("f1_score_high : ",f1_score_high)
# Load required libraries
library(tidyverse)
library(caret)
library(e1071)
# Load dataset
data <- read.csv("cleaned_tmdb_data_final.csv", stringsAsFactors = FALSE)
# Classify popularity into High, Medium, and Low
data$popularity_level <- cut(data$popularity, breaks = c(-Inf, quantile(data$popularity, c(1/3, 2/3)), Inf), labels = c("Low", "Medium", "High"))
# Remove popularity column
data <- data[, !(names(data) %in% c("popularity"))]
# Split data into training and testing sets
set.seed(123) # For reproducibility
trainIndex <- createDataPartition(data$popularity_level, p = 0.8, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]
# Train Naive Bayes classifier
model <- naiveBayes(popularity_level ~ .,
data = trainData,
laplace = 0)
# Predict using the trained model
predictions <- predict(model, testData)
# Evaluate the model
confusionMatrix(predictions, testData$popularity_level)
precision_low <-0.6178
recall_low <- 	0.9215
precision_medium <-0.5000
recall_medium <-0.3720
precision_high <-0.8929
recall_high <-	0.6826
f1_score_low <- 2 * (precision_low * recall_low) / (precision_low + recall_low)
f1_score_medium <- 2 * (precision_medium * recall_medium) / (precision_medium + recall_medium)
f1_score_high <- 2 * (precision_medium * precision_high) / (precision_medium + precision_high)
cat("f1_score_low : ",f1_score_low)
cat("f1_score_medium : ",f1_score_medium)
cat("f1_score_high : ",f1_score_high)
# Load libraries
library(tidyverse)
library(caret)
library(randomForest)
# Load dataset
data <- read.csv("cleaned_tmdb_data_final.csv", stringsAsFactors = FALSE)
# Classify popularity into High, Medium, and Low
data$popularity_level <- cut(data$popularity, breaks = c(-Inf, quantile(data$popularity, c(1/3, 2/3)), Inf), labels = c("Low", "Medium", "High"))
# Remove popularity column
data <- data[, !(names(data) %in% c("popularity"))]
# Split data into training and testing sets
set.seed(123) # For reproducibility
train_index <- createDataPartition(data$popularity_level, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Train model
rf_model <- randomForest(popularity_level ~ .,
data = train_data,
ntree = 600,
mtry = 1,
nodesize = 12,
maxnodes = 20,
maxdepth = 25)
#   - ntree: Number of trees in the forest
#   - mtry: Number of variables randomly sampled as candidates at each split
#   - nodesize: Minimum size of terminal nodes
#   - maxnodes: Maximum number of terminal nodes
#   - maxdepth: Maximum depth of the tree
# Make predictions
rf_pred <- predict(rf_model, test_data)
# Confusion matrix
confusionMatrix(rf_pred, test_data$popularity_level)
precision_low <-0.8377
recall_low <-	0.8635
precision_medium <-0.7361
recall_medium <-	0.7235
precision_high <-0.8754
recall_high <-	0.8635
f1_score_low <- 2 * (precision_low * recall_low) / (precision_low + recall_low)
f1_score_medium <- 2 * (precision_medium * recall_medium) / (precision_medium + recall_medium)
f1_score_high <- 2 * (precision_medium * precision_high) / (precision_medium + precision_high)
cat("f1_score_low : ",f1_score_low)
cat("f1_score_medium : ",f1_score_medium)
cat("f1_score_high : ",f1_score_high)
# Load libraries
library(tidyverse)
library(e1071)
library(caret)
# Load dataset
data <- read.csv("cleaned_tmdb_data_final.csv", stringsAsFactors = FALSE)
# Classify popularity into High, Medium, and Low
data$popularity_level <- cut(data$popularity, breaks = c(-Inf, quantile(data$popularity, c(1/3, 2/3)), Inf), labels = c("Low", "Medium", "High"))
# Remove popularity column
data <- data[, !(names(data) %in% c("popularity"))]
# Split data into training and testing sets
set.seed(123) # For reproducibility
train_index <- createDataPartition(data$popularity_level, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Train models
svm_model <- svm(popularity_level ~ .,
data = train_data,
kernel = "radial",
cost = 2,
epsilon = 0.1,
gamma = 0.1)
# Make predictions
svm_pred <- predict(svm_model, test_data)
# Confusion matrix
confusionMatrix(svm_pred, test_data$popularity_level)
precision_low <-0.8248
recall_low <-   	0.9317
precision_medium <-0.7962
recall_medium <-  	0.7201
precision_high <-0.9046
recall_high <-0.8737
f1_score_low <- 2 * (precision_low * recall_low) / (precision_low + recall_low)
f1_score_medium <- 2 * (precision_medium * recall_medium) / (precision_medium + recall_medium)
f1_score_high <- 2 * (precision_medium * precision_high) / (precision_medium + precision_high)
cat("f1_score_low : ",f1_score_low)
cat("f1_score_medium : ",f1_score_medium)
cat("f1_score_high : ",f1_score_high)
# Load libraries
library(tidyverse)
library(caret)
library(randomForest)
library(adabag)
# Load dataset
data <- read.csv("cleaned_tmdb_data_final.csv", stringsAsFactors = FALSE)
# Classify popularity into High, Medium, and Low
data$popularity_level <- cut(data$popularity, breaks = c(-Inf, quantile(data$popularity, c(1/3, 2/3)), Inf), labels = c("Low", "Medium", "High"))
# Remove popularity column
data <- data[, !(names(data) %in% c("popularity"))]
# Split data into training and testing sets
set.seed(123) # For reproducibility
trainIndex <- createDataPartition(data$popularity_level, p = 0.8, list = FALSE)
train_data <- data[trainIndex, ]
test_data <- data[-trainIndex, ]
# Train AdaBoost classifier
boost_model <- boosting(popularity_level ~ .,
data = train_data,
mfinal = 200,
boos = TRUE)
# Predict on test data
predictions <- predict(boost_model, newdata = test_data)
# Evaluate the model
confusionMatrix(factor(predictions$class), test_data$popularity_level)
precision_low <-0.8742
recall_low <-   	0.9010
precision_medium <-0.8107
recall_medium <-   	0.7747
precision_high <-0.8956
recall_high <-0.9078
f1_score_low <- 2 * (precision_low * recall_low) / (precision_low + recall_low)
f1_score_medium <- 2 * (precision_medium * recall_medium) / (precision_medium + recall_medium)
f1_score_high <- 2 * (precision_medium * precision_high) / (precision_medium + precision_high)
cat("f1_score_low : ",f1_score_low)
cat("f1_score_medium : ",f1_score_medium)
cat("f1_score_high : ",f1_score_high)
